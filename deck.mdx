import { themes } from 'mdx-deck'

export const theme = themes.swiss

# Why bother writing tests?

---

> The only constant is change

---

## 1. Mindset level

---

> **Quality assurance shouldn't find anything**

- Take responsibility for your inadequacies
- Take ownership for the work you do
- Minimize the number of bugs you create - constantly

---

The best role for QA is to act as specifiers and characterizers [Clean Coder]

- specifiers - gather the requirements from business and translate them into tests that describe to developers how the system should behave
- characterizers - characterize the true behavior of the system and report that to development and business

---

## 2. Theoretical level

---

Cost of fixing defects

[http://www.agilemodeling.com/essays/costOfChange.htm](http://www.agilemodeling.com/essays/costOfChange.htm)

---

## 3. Pragmatic

---

### Communication tool

The main purpose of tests is communication and specification

Testing is a bonus thing

### Confidence / Eliminate the fear of changing things

Safety net for code changes

### Allow using Continuous Delivery

---

## [XP]

You test because if you don't test, you don't know when you are done coding

You listen because if you don't listen, you don't know what to code or what to test

You design so you can keep coding and testing and listening indefinitely

---

## "... you have nothing to lose but your bugs" - [GOOS]

---

# Structure of a unit test

---

## Example 01.dbModule.test.js

---

## Structure

describe, it, test

GWT

before

after

---

## Test one isolated thing

"One coherent assertion"

Test one feature

If you are testing two features - break the test down

---

## "Start Testing with the Simplest Success Case" [GOOS]

---

## Test negative parts too

---

## Test Behavior, not methods (BDD style)

Focus on testing the features, behavior, not the API methods

You don't want to change the test description every time you add a parameter to the function

Ask yourself: is it a technical test or business test?

---

## Pick your naming style

Example:

`should connect to a database when connection string is provided`

`describe('apiCall', describe('succeeds', it('when ...)))`

---

## ğŸš¨ğŸš¨ğŸš¨ Test should be independent ğŸš¨ğŸš¨ğŸš¨

Example

---

## Learning method WTFT

---

## ğŸ¦– Exercise

ğŸš¨ Never open the "final" folder.

ğŸš¨ Disable gitlens!

ğŸ‘‰ 02.db.test.js

---

# Testing pyramid

[https://watirmelon.blog/testing-pyramids/](https://watirmelon.blog/testing-pyramids/)

---

## It all depends

ğŸ¤·

---

## Static code analysis

ESLint + Prettier + TS

---

## Unit

ğŸ’¡ Make sure that a certain unit (aka subject under test) of your codebase works as intended

---

## What is an unit?

You define it

Function / Method / Class / Module / Combination of functions

The thing you put in the `describe` function

---

## Solitary vs Sociable

![](https://martinfowler.com/bliki/images/unitTest/isolate.png)

---

Make no attempt to go solitary unless communicating with the collaborators is awkward

---

## ğŸº Grey area number 1

---

## Integration

ğŸ’¡ Make sure that the integration of your application with all the parts that live outside of your application work as intended

ğŸ’¡ Integration tests determine if independently developed units of software work correctly when they are connected to each other. [Fowler]

---

For some people, integration testing is a very broad activity that tests through a lot of different parts of your entire system. For me it's a rather narrow thing, only testing the integration with one external part at a time.

Some call them integration tests, some refer to them as component tests, some prefer the term service test.

[Fowler]

---

ğŸ“ Draw an example

---

There are lots of useful integration testing tools that throttle network bandwidth, introduce network lag, produce network errors, and otherwise test lots of other conditions that are impossible to test using unit tests which mock away the communication layer.

[Fowler]

---

Downside to mocking external dependencies: How can we ensure that the fake server we set up behaves like the real server?

With the current implementation, the separate service could change its API and our tests would still pass.

A solution to this dilemma: Running contract tests against the fake and the real server ensures that the fake we use in our integration tests is a faithful test double.

[Fowler]

---

## ğŸº Grey area number 2

---

## E2E / functional / acceptance

---

### ğŸ’¡ Functional - functional testing is a quality assurance process and a type of black-box testing that bases its test cases on the specifications of the software component under test. [Wikipedia]

Functional tests are written from the userâ€™s perspective and focus on system behavior that users are interested in.

### ğŸ’¡ Acceptance - the functional testing of a user story by the software development team during the implementation phase [Wikipedia]

Used in Aglile, mostly XP

---

#### ğŸ’¡ Ensure that the units work together as a whole system

#### ğŸ’¡ Translate requirements to tests

  1. Use DSL like [cucumber.io](https://cucumber.io/)
  2. Use JavaScript

---

E2E testing often means driving your tests through the user interface. The inverse is not true.

You can unit test React components

---

UI aka clickable tests
- Fail for unexpected reasons
- Harder to set up
- Browser quirks
- Timing issues
- Animations
- Require a lot of maintenance
- Run pretty slowly
- Sometimes impossible to run locally
  - Microservices

---

ğŸ‘‰ "REST API or a command line interface is as much of a user interface as a fancy web user interface."

API:
- More reliable
- A lot faster

---

### Due to the high maintenance cost, you should aim to reduce the number of end-to-end tests to a bare minimum

Keep GUI tests to a minimum. They are fragile because the GUI is volatile. The more GUI tests you have the less likely you are to keep them. [Clean Coder]

ğŸ‘ Just a rule of thumb

Example: validation

---

ğŸ‘ Implement the high-value/core-value test for the interactions users will have with your application

---

ğŸ‘® Do not repeat lower-level (unit, integration) test on the higher-level

---

## Manual

ğŸ‘

---

Manual

E2E / functional / acceptance

ğŸº Grey area number 2 / example

Integration

ğŸº Grey area number 1 / example

Unit

Static code analysis: ESLint + Prettier + TS

---

[GOOS]

- unit - do our objects do the right thing, are they convenient to work with?
- integration - does our code works against code we can't change?
- acceptance - does the whole system work?
  - e2e api
  - e2e gui

---

## Contract

  Snapshot / https://kentcdodds.com/blog/effective-snapshot-testing

---

## Exploratory

Try to break things, provoke issues and errors

Try to understand how things work

Example: API

Exploratory testing is particularly suitable if requirements and specifications are incomplete [Wikipedia]

---

## Smoke/Sanity testing

Just check if the page renders ğŸ¤·

---

## Regression

Regression Testing is nothing but a full or partial selection of already executed test cases which are re-executed to ensure existing functionalities work fine

---

## Non-functional

Performance

Security

---

## Usability testing

Do users know how to use the app?

---

## Naming:

- ğŸ‘ Find terms that work for you and your team
- ğŸ‘ Be clear about the different types of tests that you want to write.
- ğŸ‘ Agree on the naming in your team and find consensus on the scope of each type of test
- ğŸ‘ Get this consistent within your team/organization

---

# Mocking

---

TODO ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

---

# High-level maintenance

---

## Every code is a tech debt

---

## Every code is a tech debt

- Writing and maintaining tests takes time
- Reading and understanding other people's test takes time
- Running tests takes time
- Avoid duplicating tests throughout the different layers of the pyramid

---

## Two rules of thumb:

1. If a higher-level test spots an error and there's no lower-level test failing, you need to write a lower-level test
2. Push your tests as far down the test pyramid as you can

---

### If a higher-level test spots an error and there's no lower-level test failing, you need to write a lower-level test

The first rule is important because lower-level tests allow you to better narrow down errors and replicate them in an isolated way. They'll run faster and will be less bloated when you're debugging the issue at hand. And they will serve as a good regression test for the future.

[Fowler]

---

### Push your tests as far down the test pyramid as you can

The second rule is important to keep your test suite fast. If you have tested all conditions confidently on a lower-level test, there's no need to keep a higher-level test in your test suite. It just doesn't add more confidence that everything's working.

**You don't test all the conditional logic and edge cases that your lower-level tests already cover in the higher-level test again.**

[Fowler]

---

## Testing controller logic example

03-maintenance-01/01.test.js

---

1. Delete high-level tests that are already covered on a lower level (given they don't provide extra value)
1.Make sure that the higher-level test focuses on the part that the lower-level tests couldn't cover.
1. Eliminate tests that don't provide any value
1. Replace higher-level tests with lower-level tests if possible
1. Sometimes that's hard, especially if you know that coming up with a test was hard work. Beware of the sunk cost fallacy and hit the delete key. There's no reason to waste more precious time on a test that ceased to provide value.

---

## ğŸ’¡ All those rules apply to the low-level maintenance

(described later)

---

# What to test?

---

## ğŸ’¡ No tests is better than no tests

---

## High-level testing strategy - [XP] Testing strategy

- Test things that might break
- Test things that break the most
- Test to demonstrate some vital aspect of the software
- Test business logic first (functional part)
- Test vital infrastructure parts
- "Vital" === user-facing infrastructure features
- Translating exceptions to user responses
- Important middlewares (cross-cutting concerns)
- Test things that are reusable first
- ğŸ‘ Don't test the framework/library (!) / byÅ‚o - express route
- #example: ğŸ‘ Don't test trivial code (01.trivial-code.js)
  - getters
  - setters

---

> If you decide you really do know better than the team, then you have to make your point as unobtrusively as possible.

> For example, it is far better to suggest a test case that can only be implemented cleanly by fixing the design,
> than it is to just go and fix the design yourself.
>
> But it is a skill to not directly say what you see, but to say it in such a way that the team sees it, too.

> [XP]

---

## How would you test it?

ğŸ¦– Exercise

- https://demo.realworld.io/#/
- https://github.com/gothinkster/react-redux-realworld-example-app
- https://github.com/gothinkster/node-express-realworld-example-app

---

## Don't spill your guts / Don't test implementation details

ğŸ‘® ğŸš“ ğŸš¨ **One of the biggest mistakes**

ğŸ™ˆ **Root of all evil**

---

![](https://cdn.mos.cms.futurecdn.net/tbZkxWGmK3MxkrwpUwtuNV-1200-80.jpg)

---

How would you test he's alive?

---

## Test public API

### Don't test private functions,
### methods,
### components,
### whatever,

### just don't

---

## Examples

---

## Why?

1. Tests become brittle
2. Tests that are too close to the production code quickly become annoying
3. As soon as you refactor your production code your unit tests will break
  - Refactoring means changing the internal structure of your code without changing the externally visible behavior

---

## Exercise: Test observable behavior instead of implementation details

---

## "Listen to the Tests" [GOOS]

- When something is difficult to test, don't just ask yourself how to test it, but also why is it difficult
- If it's difficult now, it will be more difficult later
- It will make hard to change the design
- The Scout Rule
- Software Rots
- Expect the Unexpected when testing

---

## Exercise: ğŸ‘ Write test instead of debugging

---

## What is test/code coverage?

---

### ğŸ’¡ Code coverage is the percentage of lines of code that are executed during an automated test run

- **Function coverage**: how many of the functions defined have been called
- **Statement coverage**: how many of the statements in the program have been executed
- **Branches coverage**: how many of the branches of the control structures (if statements for instance) have been executed
- **Condition coverage**: how many of the boolean sub-expressions have been tested for a true and a false value
- **Line coverage**: how many of lines of source code have been tested

---

## Example: with JEST

---

## You don't need and you don't want 100% coverage

- You probably won't get 100%
- Sometimes you will
- 100% coverage doesn't mean that there are no bugs

"I would be suspicious of anything like 100% - it would smell of someone writing tests to make the coverage numbers happy, but not thinking about what they are doing."

[Fowler]

---

## What is "good test coverage"?

You rarely get bugs that escape into production, and

You are rarely hesitant to change some code for fear it will cause production bugs.

---

## Can you test too much?

"Sure you can.

You are testing too much if you can remove tests while still having enough. But this is a difficult thing to sense.

One sign you are testing too much is if your tests are slowing you down.

If it seems like a simple change to code causes excessively long changes to tests, that's a sign that there's a problem with the tests.

This may not be so much that you are testing too many things, but that you have duplication in your tests."

[Fowler]

---

## When to stop testing?

When you are confident the feature/code works as expected ğŸ™ƒ

---
